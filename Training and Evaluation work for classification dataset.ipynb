{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Load and split preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8068 entries, 0 to 8067\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   ID               8068 non-null   int64  \n",
      " 1   Gender           8068 non-null   object \n",
      " 2   Ever_Married     7928 non-null   object \n",
      " 3   Age              8068 non-null   int64  \n",
      " 4   Graduated        7990 non-null   object \n",
      " 5   Profession       7944 non-null   object \n",
      " 6   Work_Experience  7239 non-null   float64\n",
      " 7   Spending_Score   8068 non-null   object \n",
      " 8   Family_Size      7733 non-null   float64\n",
      " 9   Var_1            7992 non-null   object \n",
      " 10  Segmentation     8068 non-null   object \n",
      "dtypes: float64(2), int64(2), object(7)\n",
      "memory usage: 693.5+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\2023-phrase-2 data science Dan WEI\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "dataset = pd.read_csv('market_segmentation.csv')\n",
    "\n",
    "# View dataset information\n",
    "data.info()\n",
    "\n",
    "# Split the dataset\n",
    "X = data.drop('Segmentation', axis=1)  \n",
    "y = data['Segmentation']\n",
    "\n",
    "# Handling missing values ​​for numeric features\n",
    "numerical_features = ['Age', 'Work_Experience', 'Family_Size']\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X[numerical_features] = imputer.fit_transform(X[numerical_features])\n",
    "\n",
    "# Handling Missing Values ​​for Categorical Features\n",
    "categorical_features = ['Gender', 'Ever_Married', 'Graduated', 'Profession', 'Spending_Score', 'Var_1']\n",
    "X[categorical_features] = X[categorical_features].fillna(X[categorical_features].mode().iloc[0])\n",
    "\n",
    "# Onehotencoding of features of type string\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "X_encoded = pd.DataFrame(encoder.fit_transform(X[categorical_features]))\n",
    "X_encoded.columns = encoder.get_feature_names_out(categorical_features)\n",
    "X.drop(categorical_features, axis=1, inplace=True)\n",
    "X = pd.concat([X, X_encoded], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Choose an algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Train and test a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_classification = classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4： Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.2936802973977695\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_classification)\n",
    "\n",
    "print(\"Classification Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Summary\n",
    "\n",
    "Steps:\n",
    "\n",
    "Loaded and explored the dataset to understand its structure and features.\n",
    "Preprocessed the data by handling missing values and converting categorical features into numerical representation using one-hot encoding.\n",
    "Selected Logistic Regression for classification task and Linear Regression for regression task based on task requirements and available data.\n",
    "Trained the selected algorithms on preprocessed data to predict segmentation category for classifiers and numerical values for regressors.\n",
    "Evaluated the trained models using appropriate metrics: accuracy score for classification model and mean squared error (MSE) for regression model.\n",
    "\n",
    "Results: \n",
    "\n",
    "The classification model achieved an accuracy score of approximately 29.37%, indicating relatively low performance. Further improvement or alternative algorithms are needed to achieve better results.\n",
    "\n",
    "Interesting Observations:\n",
    "\n",
    "Class imbalance may contribute to the classification model struggling with certain classes more than others. Conducting exploratory data analysis (EDA) on model predictions can identify the classes the model struggles with the most. Techniques like oversampling or undersampling can address class imbalance and potentially improve model performance.\n",
    "Coefficients in the regression model reveal the relationship between features and the target variable. Interpreting the coefficients helps identify features with significant impact on predicted values. Note that coefficient interpretation depends on the algorithm and data context.\n",
    "Overall, the training and evaluation process provided insights into the data, model performance, and areas for improvement.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
